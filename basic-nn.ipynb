{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision as thv\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) 60000\n"
     ]
    }
   ],
   "source": [
    "# Doanload Data\n",
    "data_exists = os.path.isdir(\"./MNIST\")\n",
    "train = thv.datasets.MNIST('./', download=data_exists, train=True)\n",
    "val = thv.datasets.MNIST('./', download=data_exists, train=False)\n",
    "\n",
    "print(train.data.shape, len(train.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer_t:\n",
    "    def backward_check(self, dh_l1):\n",
    "        # PS: since the matrix dw is really sparse, it's good to try and visualize it\n",
    "        # for picking i,j values or try many different values\n",
    "        i, j = 7, 432\n",
    "\n",
    "        k = np.argwhere(dh_l1 == 1)[0, 1]\n",
    "        epsilon_ij = np.random.normal(0, 1)\n",
    "        epsilon = np.zeros_like(self.w)\n",
    "        epsilon[i, j] = epsilon_ij\n",
    "\n",
    "        dw_ij_num = np.matmul((self.w + epsilon), self.hl.T)[k, 0] - np.matmul((self.w - epsilon), self.hl.T)[k, 0]\n",
    "        dw_ij_den = 2 * epsilon_ij\n",
    "\n",
    "        dw_ij = dw_ij_num / dw_ij_den\n",
    "\n",
    "        return dw_ij, self.dw[i,j]\n",
    "\n",
    "    \n",
    "#     def backward_check_h(self, dh_l1, i=7):\n",
    "#         # PS: since the matrix hl is really sparse, it's good to try and visualize it\n",
    "#         # for picking i values or try many different values\n",
    "#         k = np.argwhere(dh_l1 == 1)[0, 1]\n",
    "        \n",
    "#         epsilon_i = np.random.normal(0, 1)\n",
    "#         epsilon = np.zeros_like(self.hl)\n",
    "#         epsilon[0, i] = epsilon_i\n",
    "\n",
    "#         dw_i_num = np.matmul((self.hl + epsilon), self.hl.T) - np.matmul((self.hl - epsilon), self.hl.T)\n",
    "#         dw_i_den = 2 * epsilon_i\n",
    "\n",
    "#         dw_i = dw_i_num / dw_i_den\n",
    "        \n",
    "#         return dw_i, self.hl[0, k]\n",
    "    \n",
    "#     def backward_check_b(self, dh_l1, i=7):\n",
    "#         # PS: since the matrix hl is really sparse, it's good to try and visualize it\n",
    "#         # for picking i values or try many different valuesgo\n",
    "#         k = np.argwhere(dh_l1 == 1)[0, 1]\n",
    "        \n",
    "#         epsilon_i = np.random.normal(0, 1)\n",
    "#         epsilon = np.zeros_like(self.b)\n",
    "#         epsilon[i] = epsilon_i\n",
    "\n",
    "#         dw_i_num = np.matmul((self.b + epsilon), self.hl)[k,0] - np.matmul((self.b - epsilon), self.hl)[k,0]\n",
    "#         dw_i_den = 2 * epsilon_i\n",
    "\n",
    "#         dw_i = dw_i_num / dw_i_den\n",
    "        \n",
    "#         return dw_i, self.hl[0, k]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        # useful to delete the stored backprop gradients of the\n",
    "        # previous mini-batch before you start a new mini-batch\n",
    "        # --- only used in linear layer\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_t(layer_t):\n",
    "    def __init__(self):\n",
    "        # input size\n",
    "        self.A = 784 \n",
    "        \n",
    "        # class count\n",
    "        self.C = 10\n",
    "        \n",
    "        # Fix seed\n",
    "        np.random.seed(546)\n",
    "        \n",
    "        # Define normalized w, and b\n",
    "        self.w = np.random.normal(loc=0, scale=1, size=(self.C, self.A))\n",
    "        self.w = self.w / np.linalg.norm(self.w)\n",
    "\n",
    "        self.b = np.random.normal(loc=0, scale=1, size=(self.C, 1))\n",
    "        self.b = self.b / np.linalg.norm(self.b)\n",
    "        \n",
    "        # Define placeholder gradients to be filled in backward step\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "\n",
    "    def forward(self, h_l):\n",
    "        h_l1 = np.matmul(h_l, np.transpose(self.w)) + self.b.T\n",
    "\n",
    "        # cache hË†l in forward because we will need it to compute\n",
    "        # dw in backward\n",
    "        self.hl = h_l\n",
    "        \n",
    "        return h_l1\n",
    "\n",
    "\n",
    "    def backward(self, dh_l1):\n",
    "        dh_l = np.matmul(dh_l1, self.w)\n",
    "        dw = np.matmul(dh_l1.T, self.hl)\n",
    "        db = np.matmul(dh_l1.T, np.ones([self.hl.shape[0], 1]))\n",
    "\n",
    "        self.dw, self.db = dw, db\n",
    "    \n",
    "        # notice that there is no need to cache dh_{l+1}\n",
    "        return dh_l\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        # useful to delete the stored backprop gradients of the\n",
    "        # previous mini-batch before you start a new mini-batch\n",
    "        self.dw, self.db = 0 * self.dw, 0 * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu_t(layer_t):\n",
    "    def forward(self, h_l):\n",
    "        return np.maximum(0, h_l)\n",
    "    \n",
    "    def backward(self, dh_l1):\n",
    "        x = np.array(dh_l1, copy=True)\n",
    "        x[x <= 0] = 0\n",
    "        x[x > 0] = 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_cross_entropy_t(layer_t):\n",
    "    def __init__(self):\n",
    "        # no parameters, nothing to initialize\n",
    "        self.h_l1 = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, h_l, y):\n",
    "        expd = np.exp(h_l)\n",
    "        h_l1 = expd / np.sum(expd, axis=1,keepdims=True) \n",
    "        \n",
    "        self.h_l1 = h_l1\n",
    "        self.y = y\n",
    "       \n",
    "        # compute average loss ell(y) over a mini-batch\n",
    "        BATCH_SIZE = h_l1.shape[0]\n",
    "        ell = np.sum(-np.log(h_l1[range(BATCH_SIZE), self.y])) / BATCH_SIZE\n",
    "        \n",
    "        # compute the error of predictions\n",
    "        error = np.sum(np.not_equal(self.y, np.argmax(h_l1, axis=1)))/BATCH_SIZE\n",
    "        \n",
    "        return ell, error\n",
    "\n",
    "    def backward(self):\n",
    "        # as we saw in the notes, the backprop input to the\n",
    "        # loss layer is 1, so this function does not take any\n",
    "        # arguments\n",
    "        \n",
    "        B = self.y.shape[0] # batch size\n",
    "        self.h_l1[range(B), self.y] -=1\n",
    "        \n",
    "        return self.h_l1/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(dataset, num_samples, num_classes):\n",
    "    # Get subsample by sampling\n",
    "    num_samples_per_class = int(num_samples/num_classes)\n",
    "    dataset = np.array(list(map(lambda elt: (np.array(elt[0]).flatten(),elt[1]), dataset)))\n",
    "    X, y = map(lambda x: np.array(list(x)),zip(*dataset))\n",
    "    subsamplesX, subsamplesY = None, None\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        # Get all X's where label == c\n",
    "        mask = np.argwhere(y == c)\n",
    "        class_X, class_Y = X[mask], y[mask]\n",
    "        class_X = class_X.reshape((class_X.shape[0], X.shape[1]))\n",
    "        \n",
    "        # Get all X's where label == c, inds = index array\n",
    "        inds = np.random.randint(0, high=class_Y.shape[0], size=num_samples_per_class)\n",
    "        subsamplesX = class_X[inds] if subsamplesX is None else np.concatenate([subsamplesX, class_X[inds]])\n",
    "        \n",
    "        # Convert Y to one-hot\n",
    "#         class_subsample = np.zeros((num_samples_per_class, num_classes))\n",
    "#         class_subsample[np.arange(num_samples_per_class), class_Y[inds]] = 1\n",
    "#         subsamplesY = class_subsample if subsamplesY is None else np.concatenate([subsamplesY,class_subsample])\n",
    "\n",
    "        # Get all Y's where label == c\n",
    "        subsamplesY = class_Y[inds] if subsamplesY is None else np.concatenate([subsamplesY, class_Y[inds]])\n",
    "    \n",
    "    # shuffle the arrays\n",
    "    A = subsamplesX.shape[1]\n",
    "    B = subsamplesX.shape[0]\n",
    "    shuf_inds = np.random.shuffle(np.array(range(B)))\n",
    "    return subsamplesX[shuf_inds].reshape((B,A)), subsamplesY[shuf_inds].reshape((num_samples,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot(y_i):\n",
    "#     hot_y = np.zeros((10,)) # C = 10\n",
    "#     hot_y[y_i] = 1\n",
    "#     return hot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CODE FOR CHECKING GRADIENT MATCHING #############\n",
    "\n",
    "## THIS FAILS FOR SOME REASON ##\n",
    "\n",
    "# l1 = linear_t()\n",
    "# l2 = relu_t()\n",
    "# l3 = softmax_cross_entropy_t()\n",
    "\n",
    "# h1 = l1.forward(np.array([trainX[25064]]))\n",
    "# h2 = l2.forward(h1)\n",
    "# ell, error = l3.forward(h2, np.array([trainY[25064]]))\n",
    "\n",
    "\n",
    "# dh2 = l3.backward()\n",
    "# dh1 = l2.backward(dh2)\n",
    "# dx = l1.backward(dh1)\n",
    "\n",
    "# estimate_grad, true_grad = l1.backward_check(dh1)\n",
    "\n",
    "# print('Estimated gradient = {}, true gradient = {}'.format(estimate_grad, true_grad))\n",
    "            \n",
    "#         estimate_grad, true_grad = NN.backward_check_h(dhl1, i=j)\n",
    "#         if abs(estimate_grad-true_grad) > 0.003:\n",
    "#             print('Estimated h_l1 = {}, true h_l1 = {}'.format(estimate_grad, true_grad))\n",
    "            \n",
    "#         estimate_grad, true_grad = NN.backward_check_b(dhl1, i=int(j*10/784))\n",
    "#         if abs(estimate_grad-true_grad) > 0.003:\n",
    "#             print('Estimated b = {}, true b = {}'.format(estimate_grad, true_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (30000,)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = subsample(dataset=train, num_samples=30000, num_classes=10)\n",
    "# valX, valY = subsample(dataset=val, num_samples=5000, num_classes=10)\n",
    "print(trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given true class: [7] our network outputted probabilities: [[9.99999092e-01 5.30000495e-10 4.92321024e-10 8.86641543e-07\n",
      "  4.92321024e-10 1.32360869e-08 4.92321024e-10 5.24804151e-09\n",
      "  4.92321024e-10 4.92321024e-10]], which is class: 0\n",
      "Our network fixed it by subtracting 1 from the true class and returning: [[ 9.99999092e-01  5.30000495e-10  4.92321024e-10  8.86641543e-07\n",
      "   4.92321024e-10  1.32360869e-08  4.92321024e-10 -9.99999995e-01\n",
      "   4.92321024e-10  4.92321024e-10]]\n",
      "After ReLu layer we get:: [[1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]]\n",
      "Iteration #0 has average loss:19.06541087532226 and error: 1.0\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "##################################################################\n",
    "############### TRAINING THE NEURAL NETWORK ######################\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "l1, l2, l3 = linear_t(), relu_t(), softmax_cross_entropy_t()\n",
    "net = [l1, l2, l3]\n",
    "\n",
    "# number of iterations\n",
    "iters = 50000\n",
    "# learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# mini-batch size\n",
    "batch_size = 1\n",
    "\n",
    "# all training data size\n",
    "training_data_size = trainX.shape[0]\n",
    "\n",
    "loss = []\n",
    "# train for at least \"iters\" iterations\n",
    "for t in range(iters):\n",
    "    # 1. sample a mini-batch of size bb = 32\n",
    "    # each image in the mini-batch is chosen uniformly randomly from the # training dataset\n",
    "    inds = np.random.randint(0,high=training_data_size, size=batch_size)\n",
    "    x, y = trainX[inds], trainY[inds]\n",
    "    \n",
    "\n",
    "    # 2. zero gradient buffer\n",
    "    for l in net: \n",
    "        l.zero_grad()\n",
    "        \n",
    "    # 3. forward pass\n",
    "    h1 = l1.forward(x)\n",
    "    h2 = l2.forward(h1)\n",
    "    ell, error = l3.forward(h2, y)\n",
    "    \n",
    "    print(f\"Given true class: {y} our network outputted probabilities: {l3.h_l1}, which is class: {np.argmax(l3.h_l1)}\")\n",
    "    \n",
    "    # 4. Backward pass\n",
    "    dh2 = l3.backward()\n",
    "    dh1 = l2.backward(dh2)\n",
    "    dx  = l1.backward(dh1)\n",
    "    \n",
    "    print(f\"Our network fixed it by subtracting 1 from the true class and returning: {dh2}\")\n",
    "    \n",
    "    print(f\"After ReLu layer we get:: {dh1}\")\n",
    "    \n",
    "    \n",
    "    # 5. gather backprop gradients\n",
    "    dw, db = l1.dw, l1.db\n",
    "    \n",
    "    # 6. print some quantities for logging and debugging\n",
    "    print(f\"Iteration #{t} has average loss:{ell} and error: {error}\")\n",
    "#     print(t, np.linalg.norm(dw/l1.w), np.linalg.norm(db/l1.b))\n",
    "    \n",
    "    # 7. one step of SGD\n",
    "    l1.w = l1.w - lr*dw\n",
    "    l1.b = l1.b - lr*db\n",
    "    \n",
    "    loss.append(error)\n",
    "    \n",
    "    \n",
    "#     for debugging:\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-30280f6017a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.show(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
